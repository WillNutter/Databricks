{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747a0efc-c4db-406f-8d2a-f1ac2badaf48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "menu_df = pd.read_csv(\"/Workspace/Repos/win185@ensign.edu/Databricks/data/menu_items.csv\")\n",
    "orders_df = pd.read_csv(\"/Workspace/Repos/win185@ensign.edu/Databricks/data/order_details.csv\")\n",
    "\n",
    "logging.info(f\"Loaded menu_items.csv with {len(menu_df)} rows\")\n",
    "logging.info(f\"Loaded order_details.csv with {len(orders_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249415b2-05f7-4cdb-9fb8-dbd9227a9a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Logging & Reproducibility Setup\n",
    "# =========================\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "LOG_FILE = os.path.join(LOG_DIR, f\"run_{timestamp}.log\")\n",
    "\n",
    "# -------------------------\n",
    "# Logging Configuration\n",
    "# -------------------------\n",
    "logger = logging.getLogger(\"etl_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# File handler\n",
    "file_handler = logging.FileHandler(LOG_FILE)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# -------------------------\n",
    "# Log run metadata\n",
    "# -------------------------\n",
    "logger.info(\"=== ETL RUN STARTED ===\")\n",
    "logger.info(f\"Timestamp: {timestamp}\")\n",
    "logger.info(f\"Python version: {sys.version}\")\n",
    "logger.info(f\"Platform: {platform.platform()}\")\n",
    "logger.info(f\"Working directory: {PROJECT_ROOT}\")\n",
    "logger.info(f\"Log file: {LOG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f123a4-091c-4453-a2d5-7f53c2809b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part A is already done in 2.4.\n",
    "Part B – Set Up Logging (≈15 min) which is above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4511f91b-dc2b-4f47-8f2f-daeb5985b31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Must be set before Python hashing happens\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Random seeds fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1cd6be-d5bd-4e79-92e1-0c0202819514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part C – Reproducibility Setup (≈15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f43bbf-08fd-4050-b65f-3bddb28497cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip freeze > /Workspace/Repos/win185@ensign.edu/Databricks/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a169b-6589-4d87-8ff2-d18633c6ef1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "data_dir = \"/Workspace/Repos/win185@ensign.edu/Databricks/data\"\n",
    "output_file = \"data_hashes.json\"\n",
    "\n",
    "hashes = {}\n",
    "\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "for filename in csv_files:\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    sha256_hash = hashlib.sha256()\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "\n",
    "    hashes[filename] = sha256_hash.hexdigest()\n",
    "\n",
    "# Write hashes to JSON\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "\n",
    "print(f\"Data hashes written to {output_file}\")\n",
    "print(json.dumps(hashes, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149ee961-3b3c-4a42-8a90-54e7e247a751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This generates the SHA-256 Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97ce2c6-881b-452a-8217-82f3f766f399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9735514f-f8a3-4669-9ed7-9d5182daffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c731f078-9c3b-4fa5-a419-83692ceba68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, collect_list\n",
    "\n",
    "# Ensure Spark session exists (safe in Databricks too)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Map logical table names to CSV paths\n",
    "tables = {\n",
    "    \"menu_items\": \"/Volumes/workspace/bronze/restaurant/menu_items.csv\",\n",
    "    \"order_details\": \"/Volumes/workspace/bronze/restaurant/order_details.csv\"\n",
    "}\n",
    "\n",
    "hashes = {}\n",
    "\n",
    "for name, path in tables.items():\n",
    "    # Read CSV with Spark\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    # Create deterministic row-level hash\n",
    "    row_hashes = df.select(\n",
    "        sha2(\n",
    "            concat_ws(\n",
    "                \"||\",\n",
    "                *[col(c).cast(\"string\") for c in df.columns]\n",
    "            ),\n",
    "            256\n",
    "        ).alias(\"row_hash\")\n",
    "    )\n",
    "\n",
    "    # Create deterministic dataset-level hash\n",
    "    dataset_hash = (\n",
    "        row_hashes\n",
    "        .orderBy(\"row_hash\")              # Order-independent\n",
    "        .groupBy()\n",
    "        .agg(\n",
    "            sha2(\n",
    "                concat_ws(\"\", collect_list(\"row_hash\")),\n",
    "                256\n",
    "            ).alias(\"dataset_hash\")\n",
    "        )\n",
    "        .collect()[0][\"dataset_hash\"]\n",
    "    )\n",
    "\n",
    "    hashes[name] = dataset_hash\n",
    "\n",
    "# Write hashes to a temporary JSON file\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "    tmp_path = f.name\n",
    "\n",
    "# Log artifact to MLflow\n",
    "# Write hashes to a temporary JSON file\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "    tmp_path = f.name\n",
    "\n",
    "# Log artifact to MLflow safely\n",
    "active_run = mlflow.active_run()\n",
    "if active_run is None:\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(tmp_path, artifact_path=\"data_lineage\")\n",
    "else:\n",
    "    mlflow.log_artifact(tmp_path, artifact_path=\"data_lineage\")\n",
    "\n",
    "print(\"Dataset hashes logged to MLflow:\")\n",
    "print(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2a5133-e12e-4603-9fd5-9e1b891559c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####Load from Unity Catalog → Pandas\n",
    "import pandas as pd\n",
    "\n",
    "messages_pd = spark.table(\"bronze.device_messages_raw\").toPandas()\n",
    "tests_pd = spark.table(\"bronze.rapid_step_tests_raw\").toPandas()\n",
    "# ---- device_messages_raw cleanup ----\n",
    "messages_pd.columns = messages_pd.columns.str.strip()\n",
    "\n",
    "messages_pd[\"sensor_type\"] = messages_pd[\"sensor_type\"].astype(str).str.strip()\n",
    "messages_pd[\"message_origin\"] = messages_pd[\"message_origin\"].astype(str).str.strip()\n",
    "\n",
    "messages_pd[\"timestamp\"] = pd.to_datetime(\n",
    "    messages_pd[\"timestamp\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ---- rapid_step_tests_raw cleanup ----\n",
    "tests_pd.columns = tests_pd.columns.str.strip()\n",
    "\n",
    "tests_pd[\"start_time\"] = pd.to_datetime(\n",
    "    tests_pd[\"start_time\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "tests_pd[\"total_steps\"] = (\n",
    "    pd.to_numeric(tests_pd[\"total_steps\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "####Join on device_id\n",
    "etl_df = tests_pd.merge(\n",
    "    messages_pd,\n",
    "    on=\"device_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "####Create a tidy table\n",
    "tidy_df = etl_df[[\n",
    "    \"device_id\",\n",
    "    \"start_time\",\n",
    "    \"sensor_type\",\n",
    "    \"message_origin\",\n",
    "    \"total_steps\"\n",
    "]].copy()\n",
    "\n",
    "tidy_df.rename(columns={\n",
    "    \"start_time\": \"test_start_time\",\n",
    "    \"sensor_type\": \"item_name\",\n",
    "    \"message_origin\": \"category\",\n",
    "    \"total_steps\": \"quantity\"\n",
    "}, inplace=True)\n",
    "####Top 5 “items” by quantity (sensor activity)\n",
    "top_5_items = (\n",
    "    tidy_df\n",
    "    .groupby(\"item_name\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "####“Revenue by category” → Activity by message origin\n",
    "activity_by_category = (\n",
    "    tidy_df\n",
    "    .groupby(\"category\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    ")\n",
    "####Busiest hour of day (tests started)\n",
    "tidy_df[\"hour\"] = tidy_df[\"test_start_time\"].dt.hour\n",
    "\n",
    "busiest_hour = (\n",
    "    tidy_df\n",
    "    .groupby(\"hour\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(1)\n",
    ")\n",
    "####Save results (MLflow — permission safe)\n",
    "import mlflow\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def log_df(df, name):\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:\n",
    "        df.to_csv(f.name, index=False)\n",
    "        mlflow.log_artifact(f.name, artifact_path=\"etl_metrics\")\n",
    "\n",
    "log_df(top_5_items, f\"top_5_items_{timestamp}.csv\")\n",
    "log_df(activity_by_category, f\"activity_by_category_{timestamp}.csv\")\n",
    "log_df(busiest_hour, f\"busiest_hour_{timestamp}.csv\")\n",
    "\n",
    "print(\"ETL metrics logged to MLflow artifacts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec359e2-f6ea-48d9-b2b4-ff7b2e38f173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part D – ETL with Pandas (≈40 min)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8723887841508495,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_3_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
