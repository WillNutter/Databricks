{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249415b2-05f7-4cdb-9fb8-dbd9227a9a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"runtime_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Clear handlers on re-run\n",
    "logger.handlers.clear()\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"Logger initialized (console only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f123a4-091c-4453-a2d5-7f53c2809b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part A is already done in 2.4.\n",
    "Part B – Set Up Logging (≈15 min) which is above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4511f91b-dc2b-4f47-8f2f-daeb5985b31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Must be set before Python hashing happens\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Random seeds fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1cd6be-d5bd-4e79-92e1-0c0202819514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part C – Reproducibility Setup (≈15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f43bbf-08fd-4050-b65f-3bddb28497cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a169b-6589-4d87-8ff2-d18633c6ef1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def sha256_file(path, chunk_size=8192):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97ce2c6-881b-452a-8217-82f3f766f399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9735514f-f8a3-4669-9ed7-9d5182daffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c731f078-9c3b-4fa5-a419-83692ceba68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import tempfile\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, collect_list\n",
    "\n",
    "tables = [\n",
    "    \"bronze.device_messages_raw\",\n",
    "    \"bronze.rapid_step_tests_raw\",\n",
    "]\n",
    "\n",
    "hashes = {}\n",
    "\n",
    "for table in tables:\n",
    "    df = spark.table(table)\n",
    "\n",
    "    # Deterministic row-level hash\n",
    "    row_hashes = df.select(\n",
    "        sha2(\n",
    "            concat_ws(\"||\", *[col(c).cast(\"string\") for c in df.columns]),\n",
    "            256\n",
    "        ).alias(\"row_hash\")\n",
    "    )\n",
    "\n",
    "    # Deterministic dataset-level hash\n",
    "    dataset_hash = (\n",
    "        row_hashes\n",
    "        .orderBy(\"row_hash\")\n",
    "        .groupBy()\n",
    "        .agg(\n",
    "            sha2(concat_ws(\"\", collect_list(\"row_hash\")), 256)\n",
    "            .alias(\"dataset_hash\")\n",
    "        )\n",
    "        .collect()[0][\"dataset_hash\"]\n",
    "    )\n",
    "\n",
    "    hashes[table] = dataset_hash\n",
    "\n",
    "# Log hashes to MLflow (no filesystem permissions needed)\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "    tmp_path = f.name\n",
    "\n",
    "mlflow.log_artifact(tmp_path, artifact_path=\"data_lineage\")\n",
    "\n",
    "print(\"Dataset hashes logged to MLflow:\")\n",
    "hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2a5133-e12e-4603-9fd5-9e1b891559c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####Load from Unity Catalog → Pandas\n",
    "import pandas as pd\n",
    "\n",
    "messages_pd = spark.table(\"bronze.device_messages_raw\").toPandas()\n",
    "tests_pd = spark.table(\"bronze.rapid_step_tests_raw\").toPandas()\n",
    "# ---- device_messages_raw cleanup ----\n",
    "messages_pd.columns = messages_pd.columns.str.strip()\n",
    "\n",
    "messages_pd[\"sensor_type\"] = messages_pd[\"sensor_type\"].astype(str).str.strip()\n",
    "messages_pd[\"message_origin\"] = messages_pd[\"message_origin\"].astype(str).str.strip()\n",
    "\n",
    "messages_pd[\"timestamp\"] = pd.to_datetime(\n",
    "    messages_pd[\"timestamp\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ---- rapid_step_tests_raw cleanup ----\n",
    "tests_pd.columns = tests_pd.columns.str.strip()\n",
    "\n",
    "tests_pd[\"start_time\"] = pd.to_datetime(\n",
    "    tests_pd[\"start_time\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "tests_pd[\"total_steps\"] = (\n",
    "    pd.to_numeric(tests_pd[\"total_steps\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "####Join on device_id\n",
    "etl_df = tests_pd.merge(\n",
    "    messages_pd,\n",
    "    on=\"device_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "####Create a tidy table\n",
    "tidy_df = etl_df[[\n",
    "    \"device_id\",\n",
    "    \"start_time\",\n",
    "    \"sensor_type\",\n",
    "    \"message_origin\",\n",
    "    \"total_steps\"\n",
    "]].copy()\n",
    "\n",
    "tidy_df.rename(columns={\n",
    "    \"start_time\": \"test_start_time\",\n",
    "    \"sensor_type\": \"item_name\",\n",
    "    \"message_origin\": \"category\",\n",
    "    \"total_steps\": \"quantity\"\n",
    "}, inplace=True)\n",
    "####Top 5 “items” by quantity (sensor activity)\n",
    "top_5_items = (\n",
    "    tidy_df\n",
    "    .groupby(\"item_name\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "####“Revenue by category” → Activity by message origin\n",
    "activity_by_category = (\n",
    "    tidy_df\n",
    "    .groupby(\"category\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    ")\n",
    "####Busiest hour of day (tests started)\n",
    "tidy_df[\"hour\"] = tidy_df[\"test_start_time\"].dt.hour\n",
    "\n",
    "busiest_hour = (\n",
    "    tidy_df\n",
    "    .groupby(\"hour\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(1)\n",
    ")\n",
    "####Save results (MLflow — permission safe)\n",
    "import mlflow\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def log_df(df, name):\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:\n",
    "        df.to_csv(f.name, index=False)\n",
    "        mlflow.log_artifact(f.name, artifact_path=\"etl_metrics\")\n",
    "\n",
    "log_df(top_5_items, f\"top_5_items_{timestamp}.csv\")\n",
    "log_df(activity_by_category, f\"activity_by_category_{timestamp}.csv\")\n",
    "log_df(busiest_hour, f\"busiest_hour_{timestamp}.csv\")\n",
    "\n",
    "print(\"ETL metrics logged to MLflow artifacts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec359e2-f6ea-48d9-b2b4-ff7b2e38f173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part D – ETL with Pandas (≈40 min)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5769073315742384,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_3_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
