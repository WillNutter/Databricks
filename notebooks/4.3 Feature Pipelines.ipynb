{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765be087-068b-4335-95b8-ba04aaa6ec1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Final rebuild of silver table with working distance_cm\n",
    "\n",
    "df_bronze = spark.read.table(\"workspace.bronze.stedi_step_curated\")\n",
    "df_bronze.filter(col(\"distance_cm\").isNotNull()).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a00220-55fd-4913-9f6a-10fffdd296a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.select(\"distance_cm\").distinct().orderBy(\"distance_cm\").show(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f90624-165d-4243-a761-6e36fff18b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Import Libraries and Load Your Curated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa481bb-c98b-40fc-8d9a-4941e99a8703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark = spark.table(\"workspace.silver.labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383cdd5d-1885-41cb-8e40-e8dbcc61ae4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a1c58d-61f8-4a40-bc62-99afd07d72a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM workspace.silver.labeled_step_test LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954520d2-7fdb-400e-bb5d-4f0e2cb6aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Define Your Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f37120-bab9-4184-ba10-fa515e0f78f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "feature_cols_categorical = [\"sensor_type\", \"device_id\"]\n",
    "label_col = \"step_label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460abc64-4c71-4a28-9bc5-4c48ca9e807b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Create a Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f14717-ddac-4f96-9cf0-192107da9ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define valid feature columns based on actual DataFrame columns\n",
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "feature_cols_categorical = [\"device_id\"]  # or add \"source_label\" if useful\n",
    "label_col = \"step_label\"\n",
    "\n",
    "# Define feature matrix and label vector\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "\n",
    "# Perform train/test split with stratification on label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d367147-5159-4d11-afa3-b8dd58613e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One-Hot Encoding (Preferred for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfcdcc8-f946-482c-9615-e15a6adc4161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_cols = feature_cols_categorical\n",
    "numeric_cols = feature_cols_numeric\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca59e13-6bc9-44b2-9a7b-0f6976fc5a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Build Preprocessing Steps - Scale numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d5f885-1d75-4224-8bf3-c68cdf56f5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ae45e4-720f-436d-b4a7-61166eee27cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One-hot encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215b60bd-b90d-4f7c-b3ab-36b1a19f36cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b9972f-b0f5-436a-8907-c7b736a51479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Combine into a single transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef26639-5042-46ea-a09c-e890dfd88ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646e4ff4-dc7e-4006-977c-942041fcf833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Build a Scikit-Learn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07425205-bb0d-405b-91c4-d5ada7da78db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0cc955-eaa1-4890-90a8-be91d7f38430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Fit the Pipeline and Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d260f0f-08d3-4894-837d-85a409253014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8646ccac-8f11-4383-ba56-2178c0345a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Save Your Processed Feature Set and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98912a7c-288b-4634-a2ba-e1ad32ddaaae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =============================\n",
    "# Paths\n",
    "# =============================\n",
    "REPO_ROOT = \"/Workspace/Repos/win185@ensign.edu/Databricks\"\n",
    "data_dir = os.path.join(REPO_ROOT, \"data\")\n",
    "artifacts_dir = os.path.join(REPO_ROOT, \"artifacts\")\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Load Raw Data\n",
    "# =============================\n",
    "raw_data_path = os.path.join(data_dir, \"rapid_step_tests.parquet\")\n",
    "assert os.path.exists(raw_data_path), f\"File not found: {raw_data_path}\"\n",
    "\n",
    "df = pd.read_parquet(raw_data_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# =============================\n",
    "# Feature Engineering / Injection\n",
    "# =============================\n",
    "# Numeric feature\n",
    "df[\"distance_cm\"] = pd.to_numeric(df.get(\"step_points\", 0), errors=\"coerce\") * 10\n",
    "\n",
    "# Categorical placeholders (required by pipeline)\n",
    "df[\"step_label\"] = \"step\"\n",
    "df[\"source_label\"] = \"device\"\n",
    "\n",
    "# Ensure categorical column exists\n",
    "if \"device_id\" not in df.columns:\n",
    "    raise ValueError(\"device_id column missing from dataset\")\n",
    "\n",
    "# =============================\n",
    "# Ensure Correct Data Types\n",
    "# =============================\n",
    "df[\"device_id\"] = df[\"device_id\"].astype(str)\n",
    "df[\"step_label\"] = df[\"step_label\"].astype(str)\n",
    "df[\"source_label\"] = df[\"source_label\"].astype(str)\n",
    "\n",
    "# =============================\n",
    "# Define Features and Target\n",
    "# =============================\n",
    "expected_features = [\n",
    "    \"device_id\",\n",
    "    \"distance_cm\",\n",
    "    \"step_label\",\n",
    "    \"source_label\"\n",
    "]\n",
    "\n",
    "# Create a copy to avoid SettingWithCopyWarning\n",
    "X = df[expected_features].copy()\n",
    "y = df[\"total_steps\"]\n",
    "\n",
    "# =============================\n",
    "# HANDLE MISSING VALUES SAFELY\n",
    "# =============================\n",
    "X[\"distance_cm\"] = X[\"distance_cm\"].fillna(0)\n",
    "X[\"device_id\"] = X[\"device_id\"].fillna(\"unknown\")\n",
    "X[\"step_label\"] = X[\"step_label\"].fillna(\"unknown\")\n",
    "X[\"source_label\"] = X[\"source_label\"].fillna(\"unknown\")\n",
    "\n",
    "# Sanity check\n",
    "assert len(X) > 0, \"Feature matrix X is empty after preprocessing\"\n",
    "\n",
    "# =============================\n",
    "# Train / Test Split\n",
    "# =============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Test split complete\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "\n",
    "# =============================\n",
    "# Load Prebuilt Pipeline\n",
    "# =============================\n",
    "pipeline_path = os.path.join(data_dir, \"stedi_feature_pipeline.pkl\")\n",
    "assert os.path.exists(pipeline_path), f\"Pipeline not found: {pipeline_path}\"\n",
    "\n",
    "pipeline = joblib.load(pipeline_path)\n",
    "\n",
    "# =============================\n",
    "# Fit and Transform\n",
    "# =============================\n",
    "pipeline.fit(X_train)\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "print(\"Pipeline fit and transform successful\")\n",
    "\n",
    "# =============================\n",
    "# Save Artifacts\n",
    "# =============================\n",
    "joblib.dump(pipeline, os.path.join(artifacts_dir, \"stedi_feature_pipeline.pkl\"))\n",
    "\n",
    "np.save(\n",
    "    os.path.join(artifacts_dir, \"X_train_transformed.npy\"),\n",
    "    X_train_transformed,\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(artifacts_dir, \"X_test_transformed.npy\"),\n",
    "    X_test_transformed,\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "pd.to_pickle(y_train, os.path.join(artifacts_dir, \"y_train.pkl\"))\n",
    "pd.to_pickle(y_test, os.path.join(artifacts_dir, \"y_test.pkl\"))\n",
    "\n",
    "print(\"\\nArtifacts saved to:\", artifacts_dir)\n",
    "print(\"Files:\", os.listdir(artifacts_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aba42fc-94c0-43b6-b15d-a160adbc594c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ethics Reflection - Using a consistent and reproducible feature pipeline helps prevent unfairness or hidden bias by ensuring that all data is processed in the same way, regardless of when or from whom it is collected. When feature generation is inconsistent, subtle differences in preprocessing can disproportionately affect certain groups and lead to biased model outcomes that are hard to detect. Reproducibility also makes it easier to audit models, trace errors, and identify where bias may have been introduced. A spiritual principle that helps illuminate the importance of consistency and fairness is the idea of treating others with impartiality and integrity—acting with the same care and standards for everyone. This perspective reinforces that fairness in machine learning is not just a technical goal, but a moral responsibility to apply rules evenly and transparently."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.3 Feature Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
