{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249415b2-05f7-4cdb-9fb8-dbd9227a9a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 01:20:58,055 | INFO | Logger initialized (console only)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"runtime_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Clear handlers on re-run\n",
    "logger.handlers.clear()\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"Logger initialized (console only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f123a4-091c-4453-a2d5-7f53c2809b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part A is already done in 2.4.\n",
    "Part B – Set Up Logging (≈15 min) which is above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4511f91b-dc2b-4f47-8f2f-daeb5985b31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds fixed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Must be set before Python hashing happens\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Random seeds fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1cd6be-d5bd-4e79-92e1-0c0202819514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part C – Reproducibility Setup (≈15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f43bbf-08fd-4050-b65f-3bddb28497cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated-types==0.7.0\nanyio==4.6.2\nargon2-cffi==21.3.0\nargon2-cffi-bindings==21.2.0\narrow==1.3.0\nasttokens==2.0.5\nastunparse==1.6.3\nasync-lru==2.0.4\nattrs==24.3.0\nazure-common==1.1.28\nazure-core==1.34.0\nazure-identity==1.20.0\nazure-mgmt-core==1.5.0\nazure-mgmt-web==8.0.0\nazure-storage-blob==12.23.0\nazure-storage-file-datalake==12.17.0\nbabel==2.16.0\nbeautifulsoup4==4.12.3\nblack==24.10.0\nbleach==6.2.0\nblinker==1.7.0\nboto3==1.36.2\nbotocore==1.36.3\ncachetools==5.5.1\ncertifi==2025.1.31\ncffi==1.17.1\nchardet==4.0.0\ncharset-normalizer==3.3.2\nclick==8.1.7\ncloudpickle==3.0.0\ncomm==0.2.1\ncontourpy==1.3.1\ncryptography==43.0.3\ncycler==0.11.0\nCython==3.0.12\ndatabricks-connect==17.2.4\ndatabricks-sdk==0.49.0\ndbus-python==1.3.2\ndebugpy==1.8.11\ndecorator==5.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.13\ndistlib==0.3.9\ndistro==1.9.0\ndistro-info==1.7+build1\ndocstring-to-markdown==0.11\nexecuting==0.8.3\nfacets-overview==1.1.1\nfastapi==0.115.12\nfastjsonschema==2.21.1\nfilelock==3.18.0\nfonttools==4.55.3\nfqdn==1.5.1\nfsspec==2023.5.0\ngitdb==4.0.11\nGitPython==3.1.43\ngoogle-api-core==2.20.0\ngoogle-auth==2.40.0\ngoogle-cloud-core==2.4.3\ngoogle-cloud-storage==3.1.0\ngoogle-crc32c==1.7.1\ngoogle-resumable-media==2.7.2\ngoogleapis-common-protos==1.65.0\ngrpcio==1.67.0\ngrpcio-status==1.67.0\nh11==0.14.0\nhttpcore==1.0.2\nhttplib2==0.20.4\nhttpx==0.27.0\nidna==3.7\nimportlib-metadata==6.6.0\niniconfig==1.1.1\nipyflow-core==0.0.209\nipykernel==6.29.5\nipython==8.30.0\nipython-genutils==0.2.0\nipywidgets==7.8.1\nisodate==0.6.1\nisoduration==20.11.0\njedi==0.19.2\nJinja2==3.1.5\njmespath==1.0.1\njoblib==1.4.2\njson5==0.9.25\njsonpointer==3.0.0\njsonschema==4.23.0\njsonschema-specifications==2023.7.1\njupyter-events==0.10.0\njupyter-lsp==2.2.0\njupyter_client==8.6.3\njupyter_core==5.7.2\njupyter_server==2.14.1\njupyter_server_terminals==0.4.4\njupyterlab==4.3.4\njupyterlab-pygments==0.1.2\njupyterlab-widgets==1.0.0\njupyterlab_server==2.27.3\nkiwisolver==1.4.8\nlaunchpadlib==1.11.0\nlazr.restfulclient==0.14.6\nlazr.uri==1.0.6\nmarkdown-it-py==2.2.0\nMarkupSafe==3.0.2\nmatplotlib==3.10.0\nmatplotlib-inline==0.1.7\nmccabe==0.7.0\nmdurl==0.1.0\nmistune==2.0.4\nmlflow-skinny==2.22.0\nmmh3==5.1.0\nmsal==1.32.3\nmsal-extensions==1.3.1\nmypy-extensions==1.0.0\nnbclient==0.8.0\nnbconvert==7.16.4\nnbformat==5.10.4\nnest-asyncio==1.6.0\nnodeenv==1.9.1\nnotebook==7.3.2\nnotebook_shim==0.2.3\nnumpy==2.1.3\noauthlib==3.2.2\nopentelemetry-api==1.32.1\nopentelemetry-sdk==1.32.1\nopentelemetry-semantic-conventions==0.53b1\noverrides==7.4.0\npackaging==24.1\npandas==2.2.3\npandocfilters==1.5.0\nparso==0.8.4\npathspec==0.10.3\npatsy==1.0.1\npexpect==4.8.0\npillow==11.1.0\nplatformdirs==3.10.0\nplotly==5.24.1\npluggy==1.5.0\nprometheus_client==0.21.0\nprompt-toolkit==3.0.43\nproto-plus==1.26.1\nprotobuf==5.29.4\npsutil==5.9.0\npsycopg2==2.9.3\nptyprocess==0.7.0\npure-eval==0.2.2\npy4j==0.10.9.9\npyarrow==19.0.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npyccolo==0.0.71\npycparser==2.21\npydantic==2.10.6\npydantic_core==2.27.2\npyflakes==3.2.0\nPygments==2.15.1\nPyGObject==3.48.2\npyiceberg==0.9.0\nPyJWT==2.10.1\npyodbc==5.2.0\npyparsing==3.2.0\npyright==1.1.394\npyspark==4.0.0+databricks.connect.17.2.2\npytest==8.3.5\npython-apt==2.7.7+ubuntu5\npython-dateutil==2.9.0.post0\npython-json-logger==3.2.1\npython-lsp-jsonrpc==1.1.2\npython-lsp-server==1.12.0\npytoolconfig==1.2.6\npytz==2024.1\nPyYAML==6.0.2\npyzmq==26.2.0\nreferencing==0.30.2\nrequests==2.32.3\nrfc3339-validator==0.1.4\nrfc3986-validator==0.1.1\nrich==13.9.4\nrope==1.12.0\nrpds-py==0.22.3\nrsa==4.9.1\ns3transfer==0.11.3\nscikit-learn==1.6.1\nscipy==1.15.1\nseaborn==0.13.2\nSend2Trash==1.8.2\nsetuptools==74.0.0\nsix==1.16.0\nsmmap==5.0.0\nsniffio==1.3.0\nsortedcontainers==2.4.0\nsoupsieve==2.5\nsqlparse==0.5.3\nssh-import-id==5.11\nstack-data==0.2.0\nstarlette==0.46.2\nstatsmodels==0.14.4\nstrictyaml==1.7.3\ntenacity==9.0.0\nterminado==0.17.1\nthreadpoolctl==3.5.0\ntinycss2==1.4.0\ntokenize_rt==6.1.0\ntomli==2.0.1\ntornado==6.4.2\ntraitlets==5.14.3\ntypes-python-dateutil==2.9.0.20241206\ntyping_extensions==4.12.2\ntzdata==2024.1\nujson==5.10.0\nunattended-upgrades==0.1\nuri-template==1.3.0\nurllib3==2.3.0\nuvicorn==0.34.2\nvirtualenv==20.29.3\nwadllib==1.3.6\nwcwidth==0.2.5\nwebcolors==24.11.1\nwebencodings==0.5.1\nwebsocket-client==1.8.0\nwhatthepatch==1.0.2\nwheel==0.45.1\nwidgetsnbextension==3.6.6\nwrapt==1.17.0\nyapf==0.40.2\nzipp==3.21.0\nzstandard==0.23.0\n"
     ]
    }
   ],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a169b-6589-4d87-8ff2-d18633c6ef1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def sha256_file(path, chunk_size=8192):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97ce2c6-881b-452a-8217-82f3f766f399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog</th></tr></thead><tbody><tr><td>bronze</td></tr><tr><td>samples</td></tr><tr><td>system</td></tr><tr><td>workspace</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "bronze"
        ],
        [
         "samples"
        ],
        [
         "system"
        ],
        [
         "workspace"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "catalog",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 22
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9735514f-f8a3-4669-9ed7-9d5182daffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>information_schema</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "information_schema"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "databaseName",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 23
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c731f078-9c3b-4fa5-a419-83692ceba68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hashes logged to MLflow:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bronze.device_messages_raw': '938e400aff171de7f5f8202a9ed4de935d0bc5c31fa0cc7724c3be2156785fd8',\n",
       " 'bronze.rapid_step_tests_raw': '6131c7c90b4f92b5e5a39df31dd696c0d08ce8b5a32e6a5b5e519e81a3769a5f'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import tempfile\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, collect_list\n",
    "\n",
    "tables = [\n",
    "    \"bronze.device_messages_raw\",\n",
    "    \"bronze.rapid_step_tests_raw\",\n",
    "]\n",
    "\n",
    "hashes = {}\n",
    "\n",
    "for table in tables:\n",
    "    df = spark.table(table)\n",
    "\n",
    "    # Deterministic row-level hash\n",
    "    row_hashes = df.select(\n",
    "        sha2(\n",
    "            concat_ws(\"||\", *[col(c).cast(\"string\") for c in df.columns]),\n",
    "            256\n",
    "        ).alias(\"row_hash\")\n",
    "    )\n",
    "\n",
    "    # Deterministic dataset-level hash\n",
    "    dataset_hash = (\n",
    "        row_hashes\n",
    "        .orderBy(\"row_hash\")\n",
    "        .groupBy()\n",
    "        .agg(\n",
    "            sha2(concat_ws(\"\", collect_list(\"row_hash\")), 256)\n",
    "            .alias(\"dataset_hash\")\n",
    "        )\n",
    "        .collect()[0][\"dataset_hash\"]\n",
    "    )\n",
    "\n",
    "    hashes[table] = dataset_hash\n",
    "\n",
    "# Log hashes to MLflow (no filesystem permissions needed)\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "    tmp_path = f.name\n",
    "\n",
    "mlflow.log_artifact(tmp_path, artifact_path=\"data_lineage\")\n",
    "\n",
    "print(\"Dataset hashes logged to MLflow:\")\n",
    "hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2a5133-e12e-4603-9fd5-9e1b891559c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL metrics logged to MLflow artifacts\n"
     ]
    }
   ],
   "source": [
    "####Load from Unity Catalog → Pandas\n",
    "import pandas as pd\n",
    "\n",
    "messages_pd = spark.table(\"bronze.device_messages_raw\").toPandas()\n",
    "tests_pd = spark.table(\"bronze.rapid_step_tests_raw\").toPandas()\n",
    "# ---- device_messages_raw cleanup ----\n",
    "messages_pd.columns = messages_pd.columns.str.strip()\n",
    "\n",
    "messages_pd[\"sensor_type\"] = messages_pd[\"sensor_type\"].astype(str).str.strip()\n",
    "messages_pd[\"message_origin\"] = messages_pd[\"message_origin\"].astype(str).str.strip()\n",
    "\n",
    "messages_pd[\"timestamp\"] = pd.to_datetime(\n",
    "    messages_pd[\"timestamp\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ---- rapid_step_tests_raw cleanup ----\n",
    "tests_pd.columns = tests_pd.columns.str.strip()\n",
    "\n",
    "tests_pd[\"start_time\"] = pd.to_datetime(\n",
    "    tests_pd[\"start_time\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "tests_pd[\"total_steps\"] = (\n",
    "    pd.to_numeric(tests_pd[\"total_steps\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "####Join on device_id\n",
    "etl_df = tests_pd.merge(\n",
    "    messages_pd,\n",
    "    on=\"device_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "####Create a tidy table\n",
    "tidy_df = etl_df[[\n",
    "    \"device_id\",\n",
    "    \"start_time\",\n",
    "    \"sensor_type\",\n",
    "    \"message_origin\",\n",
    "    \"total_steps\"\n",
    "]].copy()\n",
    "\n",
    "tidy_df.rename(columns={\n",
    "    \"start_time\": \"test_start_time\",\n",
    "    \"sensor_type\": \"item_name\",\n",
    "    \"message_origin\": \"category\",\n",
    "    \"total_steps\": \"quantity\"\n",
    "}, inplace=True)\n",
    "####Top 5 “items” by quantity (sensor activity)\n",
    "top_5_items = (\n",
    "    tidy_df\n",
    "    .groupby(\"item_name\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "####“Revenue by category” → Activity by message origin\n",
    "activity_by_category = (\n",
    "    tidy_df\n",
    "    .groupby(\"category\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    ")\n",
    "####Busiest hour of day (tests started)\n",
    "tidy_df[\"hour\"] = tidy_df[\"test_start_time\"].dt.hour\n",
    "\n",
    "busiest_hour = (\n",
    "    tidy_df\n",
    "    .groupby(\"hour\", as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity\", ascending=False)\n",
    "    .head(1)\n",
    ")\n",
    "####Save results (MLflow — permission safe)\n",
    "import mlflow\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def log_df(df, name):\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:\n",
    "        df.to_csv(f.name, index=False)\n",
    "        mlflow.log_artifact(f.name, artifact_path=\"etl_metrics\")\n",
    "\n",
    "log_df(top_5_items, f\"top_5_items_{timestamp}.csv\")\n",
    "log_df(activity_by_category, f\"activity_by_category_{timestamp}.csv\")\n",
    "log_df(busiest_hour, f\"busiest_hour_{timestamp}.csv\")\n",
    "\n",
    "print(\"ETL metrics logged to MLflow artifacts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec359e2-f6ea-48d9-b2b4-ff7b2e38f173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part D – ETL with Pandas (≈40 min)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5769073315742384,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_3_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}